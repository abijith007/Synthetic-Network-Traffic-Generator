{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import netron\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('netflow_with_geolocation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['FLOW_ID','FLOW_DURATION_MILLISECONDS', 'LAST_SWITCHED', 'TCP_WIN_MAX_IN' ,'TCP_WIN_MAX_OUT' ,'TCP_WIN_MIN_IN' ,'TCP_WIN_MIN_OUT' ,'TCP_WIN_MSS_IN' ,'TCP_WIN_SCALE_IN', 'FIRST_SWITCHED' ,'TCP_WIN_SCALE_OUT' ,'SRC_TOS' ,'DST_TOS' ,'TOTAL_FLOWS_EXP' ,'MIN_IP_PKT_LEN' ,'MAX_IP_PKT_LEN' ,'TOTAL_PKTS_EXP' ,'TOTAL_BYTES_EXP','ID'], axis=1)\n",
    "\n",
    "df.loc[df['src_country'] == 'ZZ', 'src_country'] = 'Private'\n",
    "df.loc[df['dst_country'] == 'ZZ', 'dst_country'] = 'Private'\n",
    "\n",
    "# Change city and region to 'Private' where country is now 'Private'\n",
    "df.loc[df['src_country'] == 'Private', ['src_city', 'src_region']] = 'Private'\n",
    "df.loc[df['dst_country'] == 'Private', ['dst_city', 'dst_region']] = 'Private'\n",
    "\n",
    "# Set latitude and longitude to 0 where country is 'Private'\n",
    "df.loc[df['src_country'] == 'Private', ['src_latitude', 'src_longitude']] = 0\n",
    "df.loc[df['dst_country'] == 'Private', ['dst_latitude', 'dst_longitude']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROTOCOL_MAP</th>\n",
       "      <th>L4_SRC_PORT</th>\n",
       "      <th>IPV4_SRC_ADDR</th>\n",
       "      <th>L4_DST_PORT</th>\n",
       "      <th>IPV4_DST_ADDR</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>TCP_FLAGS</th>\n",
       "      <th>IN_BYTES</th>\n",
       "      <th>IN_PKTS</th>\n",
       "      <th>OUT_BYTES</th>\n",
       "      <th>...</th>\n",
       "      <th>src_latitude</th>\n",
       "      <th>src_longitude</th>\n",
       "      <th>src_city</th>\n",
       "      <th>src_region</th>\n",
       "      <th>src_country</th>\n",
       "      <th>dst_latitude</th>\n",
       "      <th>dst_longitude</th>\n",
       "      <th>dst_city</th>\n",
       "      <th>dst_region</th>\n",
       "      <th>dst_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>udp</td>\n",
       "      <td>53950</td>\n",
       "      <td>10.114.232.40</td>\n",
       "      <td>53</td>\n",
       "      <td>10.114.226.5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tcp</td>\n",
       "      <td>37914</td>\n",
       "      <td>10.114.241.166</td>\n",
       "      <td>38303</td>\n",
       "      <td>10.114.224.218</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tcp</td>\n",
       "      <td>33216</td>\n",
       "      <td>10.114.241.166</td>\n",
       "      <td>18757</td>\n",
       "      <td>10.114.224.116</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>udp</td>\n",
       "      <td>48627</td>\n",
       "      <td>10.114.225.205</td>\n",
       "      <td>53</td>\n",
       "      <td>10.114.226.5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>udp</td>\n",
       "      <td>35939</td>\n",
       "      <td>10.114.225.205</td>\n",
       "      <td>53</td>\n",
       "      <td>10.114.226.5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PROTOCOL_MAP  L4_SRC_PORT   IPV4_SRC_ADDR  L4_DST_PORT   IPV4_DST_ADDR  \\\n",
       "0          udp        53950   10.114.232.40           53    10.114.226.5   \n",
       "1          tcp        37914  10.114.241.166        38303  10.114.224.218   \n",
       "2          tcp        33216  10.114.241.166        18757  10.114.224.116   \n",
       "3          udp        48627  10.114.225.205           53    10.114.226.5   \n",
       "4          udp        35939  10.114.225.205           53    10.114.226.5   \n",
       "\n",
       "   PROTOCOL  TCP_FLAGS  IN_BYTES  IN_PKTS  OUT_BYTES  ...  src_latitude  \\\n",
       "0        17          0       165        2        275  ...           0.0   \n",
       "1         6         22        44        1         40  ...           0.0   \n",
       "2         6         22        44        1         40  ...           0.0   \n",
       "3        17          0       128        2        160  ...           0.0   \n",
       "4        17          0       172        2        300  ...           0.0   \n",
       "\n",
       "   src_longitude  src_city src_region  src_country  dst_latitude  \\\n",
       "0            0.0   Private    Private      Private           0.0   \n",
       "1            0.0   Private    Private      Private           0.0   \n",
       "2            0.0   Private    Private      Private           0.0   \n",
       "3            0.0   Private    Private      Private           0.0   \n",
       "4            0.0   Private    Private      Private           0.0   \n",
       "\n",
       "  dst_longitude dst_city dst_region  dst_country  \n",
       "0           0.0  Private    Private      Private  \n",
       "1           0.0  Private    Private      Private  \n",
       "2           0.0  Private    Private      Private  \n",
       "3           0.0  Private    Private      Private  \n",
       "4           0.0  Private    Private      Private  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROTOCOL_MAP               0\n",
      "L4_SRC_PORT                0\n",
      "IPV4_SRC_ADDR              0\n",
      "L4_DST_PORT                0\n",
      "IPV4_DST_ADDR              0\n",
      "PROTOCOL                   0\n",
      "TCP_FLAGS                  0\n",
      "IN_BYTES                   0\n",
      "IN_PKTS                    0\n",
      "OUT_BYTES                  0\n",
      "OUT_PKTS                   0\n",
      "ANALYSIS_TIMESTAMP         0\n",
      "ANOMALY                66879\n",
      "ALERT                 128357\n",
      "src_latitude               0\n",
      "src_longitude              0\n",
      "src_city                   0\n",
      "src_region                 0\n",
      "src_country                0\n",
      "dst_latitude               0\n",
      "dst_longitude              0\n",
      "dst_city                   0\n",
      "dst_region                 0\n",
      "dst_country                0\n",
      "dtype: int64\n",
      "148071\n"
     ]
    }
   ],
   "source": [
    "missing_rows = df[df[['src_latitude', 'src_longitude', 'src_region', 'dst_region', 'dst_latitude', 'dst_longitude', 'dst_country']].isna().any(axis=1)]\n",
    "df = df.dropna(subset=['src_latitude', 'src_longitude', 'dst_latitude', 'dst_longitude', 'dst_country', 'src_region', 'dst_region'])\n",
    "print(df.isna().sum())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ANOMALY'] = pd.to_numeric(df['ANOMALY'], errors='coerce').fillna(0)\n",
    "df['ALERT'] = pd.to_numeric(df['ALERT'], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "df['src_hierarchy'] = df['src_country'] + '>' + df['src_region'] + '>' + df['src_city']\n",
    "df['dst_hierarchy'] = df['dst_country'] + '>' + df['dst_region'] + '>' + df['dst_city']\n",
    "\n",
    "\n",
    "binary_cols = ['ANOMALY', 'ALERT']\n",
    "count_cols = ['IN_BYTES', 'IN_PKTS', 'OUT_BYTES', 'OUT_PKTS']\n",
    "categorical_cols = ['PROTOCOL_MAP', 'IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'PROTOCOL', 'TCP_FLAGS', 'src_hierarchy', 'dst_hierarchy']\n",
    "numerical_cols = ['L4_SRC_PORT', 'L4_DST_PORT', 'src_latitude', 'src_longitude', 'dst_latitude', 'dst_longitude']\n",
    "\n",
    "# Setup preprocessing for numerical and categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('binary', OneHotEncoder(handle_unknown='ignore'), binary_cols),\n",
    "        ('count', FunctionTransformer(np.log1p, validate=True), count_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numerical_cols)        \n",
    "    ])\n",
    "\n",
    "# Create a preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "df_preprocessed = pipeline.fit_transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset smaller\n",
    "size = 20000\n",
    "df_preprocessed = df_preprocessed[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is sparsed\n"
     ]
    }
   ],
   "source": [
    "if issparse(df_preprocessed):\n",
    "    print(\"Data is sparsed\")\n",
    "    df_preprocessed = df_preprocessed.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, data_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(1024, activation=\"relu\", input_dim=latent_dim),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),        \n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.BatchNormalization(),        \n",
    "        layers.Dense(128, activation=\"softmax\"),\n",
    "        layers.BatchNormalization(),        \n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),        \n",
    "        layers.Dropout(0.25),        \n",
    "        layers.Dense(np.prod(data_shape), activation=\"tanh\"),\n",
    "        layers.Reshape(data_shape)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator(data_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=data_shape),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Dense(64, activation=\"softmax\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def compile_gan(generator, discriminator, learning_rate=0.0002):\n",
    "    # discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    discriminator.trainable = False  # Freeze the discriminator during generator training\n",
    "    \n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    \n",
    "    # gan_optimizer = Adam(learning_rate=learning_rate, beta_1=0.9)\n",
    "    gan_optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=gan_optimizer)\n",
    "    return gan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, gan, sparse_data, epochs, batch_size, latent_dim, callbacks=None):\n",
    "    gen_loss = []\n",
    "    disc_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        num_batches = int(np.ceil(sparse_data.shape[0] / batch_size))\n",
    "        \n",
    "        for batch_index in range(num_batches):\n",
    "            start_idx = batch_index * batch_size\n",
    "            end_idx = min((batch_index + 1) * batch_size, sparse_data.shape[0])\n",
    "            actual_batch_size = end_idx - start_idx\n",
    "            \n",
    "            batch_data = sparse_data[start_idx:end_idx]\n",
    "            noise = np.random.normal(0, 1, (actual_batch_size, latent_dim))\n",
    "            fake_data = generator.predict(noise, verbose=0)\n",
    "            real_labels = np.ones((actual_batch_size, 1))\n",
    "            fake_labels = np.zeros((actual_batch_size, 1))\n",
    "            \n",
    "            if epoch % 3 == 0:            \n",
    "                d_loss_real = discriminator.train_on_batch(batch_data, real_labels)[0]\n",
    "                d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)[0]\n",
    "                \n",
    "                d_avg_loss = (d_loss_real + d_loss_fake) / 2\n",
    "                disc_loss.append(d_avg_loss)\n",
    "            \n",
    "            g_loss = gan.train_on_batch(noise, real_labels)\n",
    "            gen_loss.append(g_loss)\n",
    "        \n",
    "        # Call callbacks manually\n",
    "        if callbacks:\n",
    "            for callback in callbacks:\n",
    "                callback.on_epoch_end(epoch, logs={'loss': d_avg_loss, 'd_loss': d_avg_loss, 'g_loss': g_loss})\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, D loss: {d_avg_loss}, G loss: {g_loss}\")\n",
    "    \n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size 32, latent dimension 500, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.90374, saving model to model_weights_32_500_0.0001.h5\n",
      "Epoch: 1, D loss: 0.5193576514720917, G loss: 0.9037403464317322\n",
      "\n",
      "Epoch 2: g_loss improved from 0.90374 to 0.51926, saving model to model_weights_32_500_0.0001.h5\n",
      "Epoch: 2, D loss: 0.5193576514720917, G loss: 0.5192552804946899\n",
      "\n",
      "Epoch 3: g_loss improved from 0.51926 to 0.51926, saving model to model_weights_32_500_0.0001.h5\n",
      "Epoch: 3, D loss: 0.5193576514720917, G loss: 0.5192550420761108\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.51926\n",
      "Epoch: 4, D loss: 0.5016881078481674, G loss: 0.9419006109237671\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.51926\n",
      "Epoch: 5, D loss: 0.5016881078481674, G loss: 0.5252731442451477\n",
      "\n",
      "Epoch 6: g_loss improved from 0.51926 to 0.50912, saving model to model_weights_32_500_0.0001.h5\n",
      "Epoch: 6, D loss: 0.5016881078481674, G loss: 0.5091181993484497\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.50912\n",
      "Epoch: 7, D loss: 0.47001153230667114, G loss: 0.9933429956436157\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.50912\n",
      "Epoch: 8, D loss: 0.47001153230667114, G loss: 0.6239795684814453\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.50912\n",
      "Epoch: 9, D loss: 0.47001153230667114, G loss: 0.5144906044006348\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.50912\n",
      "Epoch: 10, D loss: 0.43872401118278503, G loss: 1.0475250482559204\n",
      "Training with batch size 32, latent dimension 500, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.64691, saving model to model_weights_32_500_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6591404378414154, G loss: 0.6469063758850098\n",
      "\n",
      "Epoch 2: g_loss improved from 0.64691 to 0.60423, saving model to model_weights_32_500_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6591404378414154, G loss: 0.6042314767837524\n",
      "\n",
      "Epoch 3: g_loss improved from 0.60423 to 0.58343, saving model to model_weights_32_500_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6591404378414154, G loss: 0.5834251642227173\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.58343\n",
      "Epoch: 4, D loss: 0.5673469305038452, G loss: 0.8421888947486877\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.58343\n",
      "Epoch: 5, D loss: 0.5673469305038452, G loss: 0.7266664505004883\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.58343\n",
      "Epoch: 6, D loss: 0.5673469305038452, G loss: 0.7034478187561035\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.58343\n",
      "Epoch: 7, D loss: 0.5613304376602173, G loss: 0.8554993867874146\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.58343\n",
      "Epoch: 8, D loss: 0.5613304376602173, G loss: 0.8313882946968079\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.58343\n",
      "Epoch: 9, D loss: 0.5613304376602173, G loss: 0.7207776308059692\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.58343\n",
      "Epoch: 10, D loss: 0.5577549338340759, G loss: 0.8606094121932983\n",
      "Training with batch size 32, latent dimension 500, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.67992, saving model to model_weights_32_500_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6860231161117554, G loss: 0.6799209117889404\n",
      "\n",
      "Epoch 2: g_loss improved from 0.67992 to 0.67752, saving model to model_weights_32_500_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6860231161117554, G loss: 0.6775242686271667\n",
      "\n",
      "Epoch 3: g_loss improved from 0.67752 to 0.67508, saving model to model_weights_32_500_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6860231161117554, G loss: 0.6750755310058594\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.67508\n",
      "Epoch: 4, D loss: 0.6877792477607727, G loss: 0.6815363168716431\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.67508\n",
      "Epoch: 5, D loss: 0.6877792477607727, G loss: 0.6786308288574219\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.67508\n",
      "Epoch: 6, D loss: 0.6877792477607727, G loss: 0.6758068203926086\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.67508\n",
      "Epoch: 7, D loss: 0.6865008771419525, G loss: 0.6809939742088318\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.67508\n",
      "Epoch: 8, D loss: 0.6865008771419525, G loss: 0.6779085993766785\n",
      "\n",
      "Epoch 9: g_loss improved from 0.67508 to 0.67475, saving model to model_weights_32_500_1e-06.h5\n",
      "Epoch: 9, D loss: 0.6865008771419525, G loss: 0.6747527718544006\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.67475\n",
      "Epoch: 10, D loss: 0.6852532923221588, G loss: 0.6823075413703918\n",
      "Training with batch size 32, latent dimension 1000, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.89998, saving model to model_weights_32_1000_0.0001.h5\n",
      "Epoch: 1, D loss: 0.5320987701416016, G loss: 0.899975061416626\n",
      "\n",
      "Epoch 2: g_loss improved from 0.89998 to 0.54229, saving model to model_weights_32_1000_0.0001.h5\n",
      "Epoch: 2, D loss: 0.5320987701416016, G loss: 0.5422860980033875\n",
      "\n",
      "Epoch 3: g_loss improved from 0.54229 to 0.54218, saving model to model_weights_32_1000_0.0001.h5\n",
      "Epoch: 3, D loss: 0.5320987701416016, G loss: 0.542182207107544\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.54218\n",
      "Epoch: 4, D loss: 0.5139221847057343, G loss: 0.937009334564209\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.54218\n",
      "Epoch: 5, D loss: 0.5139221847057343, G loss: 0.5703543424606323\n",
      "\n",
      "Epoch 6: g_loss improved from 0.54218 to 0.53045, saving model to model_weights_32_1000_0.0001.h5\n",
      "Epoch: 6, D loss: 0.5139221847057343, G loss: 0.5304468870162964\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.53045\n",
      "Epoch: 7, D loss: 0.48701703548431396, G loss: 0.9823231101036072\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.53045\n",
      "Epoch: 8, D loss: 0.48701703548431396, G loss: 0.6094659566879272\n",
      "\n",
      "Epoch 9: g_loss improved from 0.53045 to 0.51964, saving model to model_weights_32_1000_0.0001.h5\n",
      "Epoch: 9, D loss: 0.48701703548431396, G loss: 0.519638180732727\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.51964\n",
      "Epoch: 10, D loss: 0.4548366367816925, G loss: 1.0349229574203491\n",
      "Training with batch size 32, latent dimension 1000, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.65984, saving model to model_weights_32_1000_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6569109261035919, G loss: 0.659837007522583\n",
      "\n",
      "Epoch 2: g_loss improved from 0.65984 to 0.61187, saving model to model_weights_32_1000_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6569109261035919, G loss: 0.6118676662445068\n",
      "\n",
      "Epoch 3: g_loss improved from 0.61187 to 0.58480, saving model to model_weights_32_1000_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6569109261035919, G loss: 0.5847963690757751\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.58480\n",
      "Epoch: 4, D loss: 0.5556477010250092, G loss: 0.8423305153846741\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.58480\n",
      "Epoch: 5, D loss: 0.5556477010250092, G loss: 0.7057443857192993\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.58480\n",
      "Epoch: 6, D loss: 0.5556477010250092, G loss: 0.6571156978607178\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.58480\n",
      "Epoch: 7, D loss: 0.5473909378051758, G loss: 0.8614928126335144\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.58480\n",
      "Epoch: 8, D loss: 0.5473909378051758, G loss: 0.8328505754470825\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.58480\n",
      "Epoch: 9, D loss: 0.5473909378051758, G loss: 0.7221614718437195\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.58480\n",
      "Epoch: 10, D loss: 0.5438560247421265, G loss: 0.8668420910835266\n",
      "Training with batch size 32, latent dimension 1000, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.68160, saving model to model_weights_32_1000_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6840711236000061, G loss: 0.6816028356552124\n",
      "\n",
      "Epoch 2: g_loss did not improve from 0.68160\n",
      "Epoch: 2, D loss: 0.6840711236000061, G loss: 0.6819455623626709\n",
      "\n",
      "Epoch 3: g_loss improved from 0.68160 to 0.67760, saving model to model_weights_32_1000_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6840711236000061, G loss: 0.6776039004325867\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.67760\n",
      "Epoch: 4, D loss: 0.6883952915668488, G loss: 0.6848523616790771\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.67760\n",
      "Epoch: 5, D loss: 0.6883952915668488, G loss: 0.6799328923225403\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.67760\n",
      "Epoch: 6, D loss: 0.6883952915668488, G loss: 0.6782510876655579\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.67760\n",
      "Epoch: 7, D loss: 0.6845442354679108, G loss: 0.6837401986122131\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.67760\n",
      "Epoch: 8, D loss: 0.6845442354679108, G loss: 0.6816461086273193\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.67760\n",
      "Epoch: 9, D loss: 0.6845442354679108, G loss: 0.678184986114502\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.67760\n",
      "Epoch: 10, D loss: 0.6847075819969177, G loss: 0.6860700845718384\n",
      "Training with batch size 32, latent dimension 2000, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.89990, saving model to model_weights_32_2000_0.0001.h5\n",
      "Epoch: 1, D loss: 0.5246702432632446, G loss: 0.8998990058898926\n",
      "\n",
      "Epoch 2: g_loss improved from 0.89990 to 0.52728, saving model to model_weights_32_2000_0.0001.h5\n",
      "Epoch: 2, D loss: 0.5246702432632446, G loss: 0.5272842645645142\n",
      "\n",
      "Epoch 3: g_loss improved from 0.52728 to 0.52728, saving model to model_weights_32_2000_0.0001.h5\n",
      "Epoch: 3, D loss: 0.5246702432632446, G loss: 0.5272827744483948\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.52728\n",
      "Epoch: 4, D loss: 0.49660545587539673, G loss: 0.9461724758148193\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.52728\n",
      "Epoch: 5, D loss: 0.49660545587539673, G loss: 0.6022335290908813\n",
      "\n",
      "Epoch 6: g_loss improved from 0.52728 to 0.50185, saving model to model_weights_32_2000_0.0001.h5\n",
      "Epoch: 6, D loss: 0.49660545587539673, G loss: 0.5018537640571594\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.50185\n",
      "Epoch: 7, D loss: 0.4645874798297882, G loss: 0.998923659324646\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.50185\n",
      "Epoch: 8, D loss: 0.4645874798297882, G loss: 0.6839934587478638\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.50185\n",
      "Epoch: 9, D loss: 0.4645874798297882, G loss: 0.5357982516288757\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.50185\n",
      "Epoch: 10, D loss: 0.43362845480442047, G loss: 1.0535489320755005\n",
      "Training with batch size 32, latent dimension 2000, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.65552, saving model to model_weights_32_2000_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6493098735809326, G loss: 0.6555176973342896\n",
      "\n",
      "Epoch 2: g_loss improved from 0.65552 to 0.59442, saving model to model_weights_32_2000_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6493098735809326, G loss: 0.5944170355796814\n",
      "\n",
      "Epoch 3: g_loss improved from 0.59442 to 0.56758, saving model to model_weights_32_2000_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6493098735809326, G loss: 0.5675818920135498\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.56758\n",
      "Epoch: 4, D loss: 0.5588926076889038, G loss: 0.8328726887702942\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.56758\n",
      "Epoch: 5, D loss: 0.5588926076889038, G loss: 0.7246496677398682\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.56758\n",
      "Epoch: 6, D loss: 0.5588926076889038, G loss: 0.6707630753517151\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.56758\n",
      "Epoch: 7, D loss: 0.555088460445404, G loss: 0.8454159498214722\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.56758\n",
      "Epoch: 8, D loss: 0.555088460445404, G loss: 0.8072786331176758\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.56758\n",
      "Epoch: 9, D loss: 0.555088460445404, G loss: 0.6942099928855896\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.56758\n",
      "Epoch: 10, D loss: 0.5515230596065521, G loss: 0.8504846096038818\n",
      "Training with batch size 32, latent dimension 2000, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.69196, saving model to model_weights_32_2000_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6846351623535156, G loss: 0.6919580698013306\n",
      "\n",
      "Epoch 2: g_loss improved from 0.69196 to 0.68941, saving model to model_weights_32_2000_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6846351623535156, G loss: 0.6894141435623169\n",
      "\n",
      "Epoch 3: g_loss improved from 0.68941 to 0.68637, saving model to model_weights_32_2000_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6846351623535156, G loss: 0.6863727569580078\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.68637\n",
      "Epoch: 4, D loss: 0.6874954402446747, G loss: 0.6925873756408691\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.68637\n",
      "Epoch: 5, D loss: 0.6874954402446747, G loss: 0.6891940832138062\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.68637\n",
      "Epoch: 6, D loss: 0.6874954402446747, G loss: 0.686802864074707\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.68637\n",
      "Epoch: 7, D loss: 0.6871604025363922, G loss: 0.6918025016784668\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.68637\n",
      "Epoch: 8, D loss: 0.6871604025363922, G loss: 0.6896295547485352\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.68637\n",
      "Epoch: 9, D loss: 0.6871604025363922, G loss: 0.6870537996292114\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.68637\n",
      "Epoch: 10, D loss: 0.6856152415275574, G loss: 0.6926450729370117\n",
      "Training with batch size 64, latent dimension 500, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.86113, saving model to model_weights_64_500_0.0001.h5\n",
      "Epoch: 1, D loss: 0.546874463558197, G loss: 0.8611346483230591\n",
      "\n",
      "Epoch 2: g_loss improved from 0.86113 to 0.54874, saving model to model_weights_64_500_0.0001.h5\n",
      "Epoch: 2, D loss: 0.546874463558197, G loss: 0.5487414002418518\n",
      "\n",
      "Epoch 3: g_loss did not improve from 0.54874\n",
      "Epoch: 3, D loss: 0.546874463558197, G loss: 0.5487421751022339\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.54874\n",
      "Epoch: 4, D loss: 0.7040739059448242, G loss: 0.5562822818756104\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.54874\n",
      "Epoch: 5, D loss: 0.7040739059448242, G loss: 0.5562822818756104\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.54874\n",
      "Epoch: 6, D loss: 0.7040739059448242, G loss: 0.5562822818756104\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.54874\n",
      "Epoch: 7, D loss: 0.5329608917236328, G loss: 0.9055826663970947\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.54874\n",
      "Epoch: 8, D loss: 0.5329608917236328, G loss: 0.6304013729095459\n",
      "\n",
      "Epoch 9: g_loss improved from 0.54874 to 0.54774, saving model to model_weights_64_500_0.0001.h5\n",
      "Epoch: 9, D loss: 0.5329608917236328, G loss: 0.5477381944656372\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.54774\n",
      "Epoch: 10, D loss: 0.5160658359527588, G loss: 0.9304598569869995\n",
      "Training with batch size 64, latent dimension 500, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.65430, saving model to model_weights_64_500_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6653223633766174, G loss: 0.6542962789535522\n",
      "\n",
      "Epoch 2: g_loss improved from 0.65430 to 0.63347, saving model to model_weights_64_500_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6653223633766174, G loss: 0.6334689855575562\n",
      "\n",
      "Epoch 3: g_loss improved from 0.63347 to 0.61204, saving model to model_weights_64_500_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6653223633766174, G loss: 0.612041711807251\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.61204\n",
      "Epoch: 4, D loss: 0.6122287511825562, G loss: 0.7834234237670898\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.61204\n",
      "Epoch: 5, D loss: 0.6122287511825562, G loss: 0.7214117050170898\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.61204\n",
      "Epoch: 6, D loss: 0.6122287511825562, G loss: 0.6415177583694458\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.61204\n",
      "Epoch: 7, D loss: 0.5887067914009094, G loss: 0.7953665256500244\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.61204\n",
      "Epoch: 8, D loss: 0.5887067914009094, G loss: 0.7199438810348511\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.61204\n",
      "Epoch: 9, D loss: 0.5887067914009094, G loss: 0.6378042697906494\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.61204\n",
      "Epoch: 10, D loss: 0.5703124403953552, G loss: 0.8078948259353638\n",
      "Training with batch size 64, latent dimension 500, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.69348, saving model to model_weights_64_500_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6877245306968689, G loss: 0.6934837102890015\n",
      "\n",
      "Epoch 2: g_loss improved from 0.69348 to 0.69225, saving model to model_weights_64_500_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6877245306968689, G loss: 0.6922484636306763\n",
      "\n",
      "Epoch 3: g_loss improved from 0.69225 to 0.69097, saving model to model_weights_64_500_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6877245306968689, G loss: 0.6909652352333069\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.69097\n",
      "Epoch: 4, D loss: 0.6892696917057037, G loss: 0.6954046487808228\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.69097\n",
      "Epoch: 5, D loss: 0.6892696917057037, G loss: 0.6931743025779724\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.69097\n",
      "Epoch: 6, D loss: 0.6892696917057037, G loss: 0.6918390989303589\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.69097\n",
      "Epoch: 7, D loss: 0.6878541707992554, G loss: 0.6950858235359192\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.69097\n",
      "Epoch: 8, D loss: 0.6878541707992554, G loss: 0.6929271221160889\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.69097\n",
      "Epoch: 9, D loss: 0.6878541707992554, G loss: 0.6918412446975708\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.69097\n",
      "Epoch: 10, D loss: 0.6880412697792053, G loss: 0.695850133895874\n",
      "Training with batch size 64, latent dimension 1000, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.82331, saving model to model_weights_64_1000_0.0001.h5\n",
      "Epoch: 1, D loss: 0.5442753434181213, G loss: 0.8233141303062439\n",
      "\n",
      "Epoch 2: g_loss improved from 0.82331 to 0.54912, saving model to model_weights_64_1000_0.0001.h5\n",
      "Epoch: 2, D loss: 0.5442753434181213, G loss: 0.5491166710853577\n",
      "\n",
      "Epoch 3: g_loss improved from 0.54912 to 0.54912, saving model to model_weights_64_1000_0.0001.h5\n",
      "Epoch: 3, D loss: 0.5442753434181213, G loss: 0.5491162538528442\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.54912\n",
      "Epoch: 4, D loss: 0.5349362790584564, G loss: 0.8945251107215881\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.54912\n",
      "Epoch: 5, D loss: 0.5349362790584564, G loss: 0.6242339015007019\n",
      "\n",
      "Epoch 6: g_loss improved from 0.54912 to 0.54412, saving model to model_weights_64_1000_0.0001.h5\n",
      "Epoch: 6, D loss: 0.5349362790584564, G loss: 0.5441211462020874\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.54412\n",
      "Epoch: 7, D loss: 0.5184659361839294, G loss: 0.9189311265945435\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.54412\n",
      "Epoch: 8, D loss: 0.5184659361839294, G loss: 0.6552313566207886\n",
      "\n",
      "Epoch 9: g_loss improved from 0.54412 to 0.52789, saving model to model_weights_64_1000_0.0001.h5\n",
      "Epoch: 9, D loss: 0.5184659361839294, G loss: 0.5278931260108948\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.52789\n",
      "Epoch: 10, D loss: 0.5015659630298615, G loss: 0.9447431564331055\n",
      "Training with batch size 64, latent dimension 1000, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.69688, saving model to model_weights_64_1000_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6661038398742676, G loss: 0.696884274482727\n",
      "\n",
      "Epoch 2: g_loss improved from 0.69688 to 0.67128, saving model to model_weights_64_1000_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6661038398742676, G loss: 0.6712799072265625\n",
      "\n",
      "Epoch 3: g_loss improved from 0.67128 to 0.64613, saving model to model_weights_64_1000_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6661038398742676, G loss: 0.6461312770843506\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.64613\n",
      "Epoch: 4, D loss: 0.6239184439182281, G loss: 0.8208535313606262\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.64613\n",
      "Epoch: 5, D loss: 0.6239184439182281, G loss: 0.740256667137146\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.64613\n",
      "Epoch: 6, D loss: 0.6239184439182281, G loss: 0.6841855049133301\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.64613\n",
      "Epoch: 7, D loss: 0.5853969752788544, G loss: 0.8142324686050415\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.64613\n",
      "Epoch: 8, D loss: 0.5853969752788544, G loss: 0.7255241274833679\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.64613\n",
      "Epoch: 9, D loss: 0.5853969752788544, G loss: 0.6623815298080444\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.64613\n",
      "Epoch: 10, D loss: 0.5624576807022095, G loss: 0.825550377368927\n",
      "Training with batch size 64, latent dimension 1000, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.70745, saving model to model_weights_64_1000_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6888644993305206, G loss: 0.7074483633041382\n",
      "\n",
      "Epoch 2: g_loss improved from 0.70745 to 0.70623, saving model to model_weights_64_1000_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6888644993305206, G loss: 0.7062317132949829\n",
      "\n",
      "Epoch 3: g_loss improved from 0.70623 to 0.70529, saving model to model_weights_64_1000_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6888644993305206, G loss: 0.7052886486053467\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.70529\n",
      "Epoch: 4, D loss: 0.6906912326812744, G loss: 0.7093226909637451\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.70529\n",
      "Epoch: 5, D loss: 0.6906912326812744, G loss: 0.7066579461097717\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.70529\n",
      "Epoch: 6, D loss: 0.6906912326812744, G loss: 0.7055974006652832\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.70529\n",
      "Epoch: 7, D loss: 0.6904287040233612, G loss: 0.7085467576980591\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.70529\n",
      "Epoch: 8, D loss: 0.6904287040233612, G loss: 0.7083307504653931\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.70529\n",
      "Epoch: 9, D loss: 0.6904287040233612, G loss: 0.70639967918396\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.70529\n",
      "Epoch: 10, D loss: 0.688815712928772, G loss: 0.709945797920227\n",
      "Training with batch size 64, latent dimension 2000, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.82141, saving model to model_weights_64_2000_0.0001.h5\n",
      "Epoch: 1, D loss: 0.5397070348262787, G loss: 0.8214145302772522\n",
      "\n",
      "Epoch 2: g_loss improved from 0.82141 to 0.53995, saving model to model_weights_64_2000_0.0001.h5\n",
      "Epoch: 2, D loss: 0.5397070348262787, G loss: 0.5399467349052429\n",
      "\n",
      "Epoch 3: g_loss improved from 0.53995 to 0.53995, saving model to model_weights_64_2000_0.0001.h5\n",
      "Epoch: 3, D loss: 0.5397070348262787, G loss: 0.5399457216262817\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.53995\n",
      "Epoch: 4, D loss: 0.5325740575790405, G loss: 0.8946701884269714\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.53995\n",
      "Epoch: 5, D loss: 0.5325740575790405, G loss: 0.6173473596572876\n",
      "\n",
      "Epoch 6: g_loss improved from 0.53995 to 0.53949, saving model to model_weights_64_2000_0.0001.h5\n",
      "Epoch: 6, D loss: 0.5325740575790405, G loss: 0.5394885540008545\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.53949\n",
      "Epoch: 7, D loss: 0.5156694054603577, G loss: 0.9197003841400146\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.53949\n",
      "Epoch: 8, D loss: 0.5156694054603577, G loss: 0.6473633050918579\n",
      "\n",
      "Epoch 9: g_loss improved from 0.53949 to 0.52265, saving model to model_weights_64_2000_0.0001.h5\n",
      "Epoch: 9, D loss: 0.5156694054603577, G loss: 0.522651195526123\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.52265\n",
      "Epoch: 10, D loss: 0.4988255053758621, G loss: 0.9453965425491333\n",
      "Training with batch size 64, latent dimension 2000, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.66702, saving model to model_weights_64_2000_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6688980460166931, G loss: 0.6670244336128235\n",
      "\n",
      "Epoch 2: g_loss improved from 0.66702 to 0.64447, saving model to model_weights_64_2000_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6688980460166931, G loss: 0.6444716453552246\n",
      "\n",
      "Epoch 3: g_loss improved from 0.64447 to 0.62209, saving model to model_weights_64_2000_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6688980460166931, G loss: 0.6220861673355103\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.62209\n",
      "Epoch: 4, D loss: 0.6254332661628723, G loss: 0.7785756587982178\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.62209\n",
      "Epoch: 5, D loss: 0.6254332661628723, G loss: 0.6953603029251099\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.62209\n",
      "Epoch: 6, D loss: 0.6254332661628723, G loss: 0.6506785154342651\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.62209\n",
      "Epoch: 7, D loss: 0.5873346030712128, G loss: 0.7881366014480591\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.62209\n",
      "Epoch: 8, D loss: 0.5873346030712128, G loss: 0.6864781379699707\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.62209\n",
      "Epoch: 9, D loss: 0.5873346030712128, G loss: 0.6388635635375977\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.62209\n",
      "Epoch: 10, D loss: 0.5633285343647003, G loss: 0.8220417499542236\n",
      "Training with batch size 64, latent dimension 2000, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.67866, saving model to model_weights_64_2000_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6883673965930939, G loss: 0.6786625385284424\n",
      "\n",
      "Epoch 2: g_loss improved from 0.67866 to 0.67677, saving model to model_weights_64_2000_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6883673965930939, G loss: 0.6767683029174805\n",
      "\n",
      "Epoch 3: g_loss improved from 0.67677 to 0.67593, saving model to model_weights_64_2000_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6883673965930939, G loss: 0.6759320497512817\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.67593\n",
      "Epoch: 4, D loss: 0.6902266144752502, G loss: 0.6804436445236206\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.67593\n",
      "Epoch: 5, D loss: 0.6902266144752502, G loss: 0.6781595945358276\n",
      "\n",
      "Epoch 6: g_loss did not improve from 0.67593\n",
      "Epoch: 6, D loss: 0.6902266144752502, G loss: 0.6770522594451904\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.67593\n",
      "Epoch: 7, D loss: 0.6890525221824646, G loss: 0.6818984150886536\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.67593\n",
      "Epoch: 8, D loss: 0.6890525221824646, G loss: 0.6782006025314331\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.67593\n",
      "Epoch: 9, D loss: 0.6890525221824646, G loss: 0.6781579256057739\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.67593\n",
      "Epoch: 10, D loss: 0.6882095336914062, G loss: 0.6804347038269043\n",
      "Training with batch size 128, latent dimension 500, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.56764, saving model to model_weights_128_500_0.0001.h5\n",
      "Epoch: 1, D loss: 0.5995422303676605, G loss: 0.5676422119140625\n",
      "\n",
      "Epoch 2: g_loss improved from 0.56764 to 0.55059, saving model to model_weights_128_500_0.0001.h5\n",
      "Epoch: 2, D loss: 0.5995422303676605, G loss: 0.5505870580673218\n",
      "\n",
      "Epoch 3: g_loss improved from 0.55059 to 0.55058, saving model to model_weights_128_500_0.0001.h5\n",
      "Epoch: 3, D loss: 0.5995422303676605, G loss: 0.5505794286727905\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.55058\n",
      "Epoch: 4, D loss: 0.5436174273490906, G loss: 0.7610651254653931\n",
      "\n",
      "Epoch 5: g_loss improved from 0.55058 to 0.54531, saving model to model_weights_128_500_0.0001.h5\n",
      "Epoch: 5, D loss: 0.5436174273490906, G loss: 0.5453087091445923\n",
      "\n",
      "Epoch 6: g_loss improved from 0.54531 to 0.54520, saving model to model_weights_128_500_0.0001.h5\n",
      "Epoch: 6, D loss: 0.5436174273490906, G loss: 0.5451958775520325\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.54520\n",
      "Epoch: 7, D loss: 0.5369758903980255, G loss: 0.8825148344039917\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.54520\n",
      "Epoch: 8, D loss: 0.5369758903980255, G loss: 0.67225581407547\n",
      "\n",
      "Epoch 9: g_loss improved from 0.54520 to 0.54034, saving model to model_weights_128_500_0.0001.h5\n",
      "Epoch: 9, D loss: 0.5369758903980255, G loss: 0.5403361320495605\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.54034\n",
      "Epoch: 10, D loss: 0.5284195840358734, G loss: 0.8950456380844116\n",
      "Training with batch size 128, latent dimension 500, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.67687, saving model to model_weights_128_500_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6862335503101349, G loss: 0.6768708229064941\n",
      "\n",
      "Epoch 2: g_loss improved from 0.67687 to 0.66566, saving model to model_weights_128_500_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6862335503101349, G loss: 0.6656585931777954\n",
      "\n",
      "Epoch 3: g_loss improved from 0.66566 to 0.65527, saving model to model_weights_128_500_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6862335503101349, G loss: 0.6552659869194031\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.65527\n",
      "Epoch: 4, D loss: 0.6653044521808624, G loss: 0.6637308597564697\n",
      "\n",
      "Epoch 5: g_loss improved from 0.65527 to 0.64869, saving model to model_weights_128_500_1e-05.h5\n",
      "Epoch: 5, D loss: 0.6653044521808624, G loss: 0.6486941576004028\n",
      "\n",
      "Epoch 6: g_loss improved from 0.64869 to 0.63800, saving model to model_weights_128_500_1e-05.h5\n",
      "Epoch: 6, D loss: 0.6653044521808624, G loss: 0.6380048394203186\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.63800\n",
      "Epoch: 7, D loss: 0.6659223437309265, G loss: 0.7189575433731079\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.63800\n",
      "Epoch: 8, D loss: 0.6659223437309265, G loss: 0.675649881362915\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.63800\n",
      "Epoch: 9, D loss: 0.6659223437309265, G loss: 0.6608855128288269\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.63800\n",
      "Epoch: 10, D loss: 0.632969856262207, G loss: 0.7226861715316772\n",
      "Training with batch size 128, latent dimension 500, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.68432, saving model to model_weights_128_500_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6917314231395721, G loss: 0.6843162775039673\n",
      "\n",
      "Epoch 2: g_loss improved from 0.68432 to 0.68390, saving model to model_weights_128_500_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6917314231395721, G loss: 0.6839009523391724\n",
      "\n",
      "Epoch 3: g_loss improved from 0.68390 to 0.68266, saving model to model_weights_128_500_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6917314231395721, G loss: 0.6826594471931458\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.68266\n",
      "Epoch: 4, D loss: 0.687462329864502, G loss: 0.6842252612113953\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.68266\n",
      "Epoch: 5, D loss: 0.687462329864502, G loss: 0.6833240985870361\n",
      "\n",
      "Epoch 6: g_loss improved from 0.68266 to 0.68216, saving model to model_weights_128_500_1e-06.h5\n",
      "Epoch: 6, D loss: 0.687462329864502, G loss: 0.6821642518043518\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.68216\n",
      "Epoch: 7, D loss: 0.6904072463512421, G loss: 0.6841671466827393\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.68216\n",
      "Epoch: 8, D loss: 0.6904072463512421, G loss: 0.6841691732406616\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.68216\n",
      "Epoch: 9, D loss: 0.6904072463512421, G loss: 0.6824827194213867\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.68216\n",
      "Epoch: 10, D loss: 0.6886515021324158, G loss: 0.6856638193130493\n",
      "Training with batch size 128, latent dimension 1000, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.56003, saving model to model_weights_128_1000_0.0001.h5\n",
      "Epoch: 1, D loss: 0.6045107543468475, G loss: 0.5600306987762451\n",
      "\n",
      "Epoch 2: g_loss improved from 0.56003 to 0.54542, saving model to model_weights_128_1000_0.0001.h5\n",
      "Epoch: 2, D loss: 0.6045107543468475, G loss: 0.5454245805740356\n",
      "\n",
      "Epoch 3: g_loss improved from 0.54542 to 0.54534, saving model to model_weights_128_1000_0.0001.h5\n",
      "Epoch: 3, D loss: 0.6045107543468475, G loss: 0.5453430414199829\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.54534\n",
      "Epoch: 4, D loss: 0.5381665229797363, G loss: 0.7784392833709717\n",
      "\n",
      "Epoch 5: g_loss improved from 0.54534 to 0.53787, saving model to model_weights_128_1000_0.0001.h5\n",
      "Epoch: 5, D loss: 0.5381665229797363, G loss: 0.5378743410110474\n",
      "\n",
      "Epoch 6: g_loss improved from 0.53787 to 0.53785, saving model to model_weights_128_1000_0.0001.h5\n",
      "Epoch: 6, D loss: 0.5381665229797363, G loss: 0.5378482341766357\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.53785\n",
      "Epoch: 7, D loss: 0.5325141847133636, G loss: 0.886927604675293\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.53785\n",
      "Epoch: 8, D loss: 0.5325141847133636, G loss: 0.6485803127288818\n",
      "\n",
      "Epoch 9: g_loss improved from 0.53785 to 0.53580, saving model to model_weights_128_1000_0.0001.h5\n",
      "Epoch: 9, D loss: 0.5325141847133636, G loss: 0.5357983112335205\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.53580\n",
      "Epoch: 10, D loss: 0.5240840315818787, G loss: 0.8994977474212646\n",
      "Training with batch size 128, latent dimension 1000, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.68212, saving model to model_weights_128_1000_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6859601736068726, G loss: 0.6821223497390747\n",
      "\n",
      "Epoch 2: g_loss improved from 0.68212 to 0.67388, saving model to model_weights_128_1000_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6859601736068726, G loss: 0.6738757491111755\n",
      "\n",
      "Epoch 3: g_loss improved from 0.67388 to 0.66584, saving model to model_weights_128_1000_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6859601736068726, G loss: 0.6658419966697693\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.66584\n",
      "Epoch: 4, D loss: 0.6646142601966858, G loss: 0.670154333114624\n",
      "\n",
      "Epoch 5: g_loss improved from 0.66584 to 0.65717, saving model to model_weights_128_1000_1e-05.h5\n",
      "Epoch: 5, D loss: 0.6646142601966858, G loss: 0.6571729183197021\n",
      "\n",
      "Epoch 6: g_loss improved from 0.65717 to 0.64534, saving model to model_weights_128_1000_1e-05.h5\n",
      "Epoch: 6, D loss: 0.6646142601966858, G loss: 0.6453413963317871\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.64534\n",
      "Epoch: 7, D loss: 0.6622814238071442, G loss: 0.7070335149765015\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.64534\n",
      "Epoch: 8, D loss: 0.6622814238071442, G loss: 0.6875500082969666\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.64534\n",
      "Epoch: 9, D loss: 0.6622814238071442, G loss: 0.670500636100769\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.64534\n",
      "Epoch: 10, D loss: 0.639515221118927, G loss: 0.7365769147872925\n",
      "Training with batch size 128, latent dimension 1000, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.69606, saving model to model_weights_128_1000_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6914094090461731, G loss: 0.6960599422454834\n",
      "\n",
      "Epoch 2: g_loss improved from 0.69606 to 0.69553, saving model to model_weights_128_1000_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6914094090461731, G loss: 0.6955281496047974\n",
      "\n",
      "Epoch 3: g_loss improved from 0.69553 to 0.69443, saving model to model_weights_128_1000_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6914094090461731, G loss: 0.6944320201873779\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.69443\n",
      "Epoch: 4, D loss: 0.6883498132228851, G loss: 0.695041835308075\n",
      "\n",
      "Epoch 5: g_loss improved from 0.69443 to 0.69434, saving model to model_weights_128_1000_1e-06.h5\n",
      "Epoch: 5, D loss: 0.6883498132228851, G loss: 0.6943379640579224\n",
      "\n",
      "Epoch 6: g_loss improved from 0.69434 to 0.69387, saving model to model_weights_128_1000_1e-06.h5\n",
      "Epoch: 6, D loss: 0.6883498132228851, G loss: 0.6938673257827759\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.69387\n",
      "Epoch: 7, D loss: 0.6889707446098328, G loss: 0.6960917115211487\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.69387\n",
      "Epoch: 8, D loss: 0.6889707446098328, G loss: 0.6947379112243652\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.69387\n",
      "Epoch: 9, D loss: 0.6889707446098328, G loss: 0.6940593719482422\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.69387\n",
      "Epoch: 10, D loss: 0.6897355616092682, G loss: 0.6964398622512817\n",
      "Training with batch size 128, latent dimension 2000, and learning rate 0.0001\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.56256, saving model to model_weights_128_2000_0.0001.h5\n",
      "Epoch: 1, D loss: 0.610698938369751, G loss: 0.562557578086853\n",
      "\n",
      "Epoch 2: g_loss improved from 0.56256 to 0.55598, saving model to model_weights_128_2000_0.0001.h5\n",
      "Epoch: 2, D loss: 0.610698938369751, G loss: 0.5559771060943604\n",
      "\n",
      "Epoch 3: g_loss improved from 0.55598 to 0.55597, saving model to model_weights_128_2000_0.0001.h5\n",
      "Epoch: 3, D loss: 0.610698938369751, G loss: 0.5559678077697754\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.55597\n",
      "Epoch: 4, D loss: 0.5451518297195435, G loss: 0.7985882759094238\n",
      "\n",
      "Epoch 5: g_loss improved from 0.55597 to 0.54849, saving model to model_weights_128_2000_0.0001.h5\n",
      "Epoch: 5, D loss: 0.5451518297195435, G loss: 0.5484880208969116\n",
      "\n",
      "Epoch 6: g_loss improved from 0.54849 to 0.54820, saving model to model_weights_128_2000_0.0001.h5\n",
      "Epoch: 6, D loss: 0.5451518297195435, G loss: 0.5481977462768555\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.54820\n",
      "Epoch: 7, D loss: 0.5383626818656921, G loss: 0.8824512958526611\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.54820\n",
      "Epoch: 8, D loss: 0.5383626818656921, G loss: 0.6929194927215576\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.54820\n",
      "Epoch: 9, D loss: 0.5383626818656921, G loss: 0.5531768798828125\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.54820\n",
      "Epoch: 10, D loss: 0.5297411978244781, G loss: 0.8949825763702393\n",
      "Training with batch size 128, latent dimension 2000, and learning rate 1e-05\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.69742, saving model to model_weights_128_2000_1e-05.h5\n",
      "Epoch: 1, D loss: 0.6855116784572601, G loss: 0.6974180340766907\n",
      "\n",
      "Epoch 2: g_loss improved from 0.69742 to 0.68731, saving model to model_weights_128_2000_1e-05.h5\n",
      "Epoch: 2, D loss: 0.6855116784572601, G loss: 0.6873129606246948\n",
      "\n",
      "Epoch 3: g_loss improved from 0.68731 to 0.67052, saving model to model_weights_128_2000_1e-05.h5\n",
      "Epoch: 3, D loss: 0.6855116784572601, G loss: 0.6705174446105957\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.67052\n",
      "Epoch: 4, D loss: 0.6607836782932281, G loss: 0.6777464151382446\n",
      "\n",
      "Epoch 5: g_loss improved from 0.67052 to 0.65545, saving model to model_weights_128_2000_1e-05.h5\n",
      "Epoch: 5, D loss: 0.6607836782932281, G loss: 0.6554538011550903\n",
      "\n",
      "Epoch 6: g_loss improved from 0.65545 to 0.64925, saving model to model_weights_128_2000_1e-05.h5\n",
      "Epoch: 6, D loss: 0.6607836782932281, G loss: 0.6492524743080139\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.64925\n",
      "Epoch: 7, D loss: 0.6596333682537079, G loss: 0.7235621809959412\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.64925\n",
      "Epoch: 8, D loss: 0.6596333682537079, G loss: 0.6952475905418396\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.64925\n",
      "Epoch: 9, D loss: 0.6596333682537079, G loss: 0.6776508092880249\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.64925\n",
      "Epoch: 10, D loss: 0.6309193074703217, G loss: 0.7355459928512573\n",
      "Training with batch size 128, latent dimension 2000, and learning rate 1e-06\n",
      "\n",
      "Epoch 1: g_loss improved from inf to 0.70021, saving model to model_weights_128_2000_1e-06.h5\n",
      "Epoch: 1, D loss: 0.6917477548122406, G loss: 0.7002123594284058\n",
      "\n",
      "Epoch 2: g_loss improved from 0.70021 to 0.69931, saving model to model_weights_128_2000_1e-06.h5\n",
      "Epoch: 2, D loss: 0.6917477548122406, G loss: 0.6993069648742676\n",
      "\n",
      "Epoch 3: g_loss improved from 0.69931 to 0.69870, saving model to model_weights_128_2000_1e-06.h5\n",
      "Epoch: 3, D loss: 0.6917477548122406, G loss: 0.6987018585205078\n",
      "\n",
      "Epoch 4: g_loss did not improve from 0.69870\n",
      "Epoch: 4, D loss: 0.6889429688453674, G loss: 0.6994178295135498\n",
      "\n",
      "Epoch 5: g_loss did not improve from 0.69870\n",
      "Epoch: 5, D loss: 0.6889429688453674, G loss: 0.6988034844398499\n",
      "\n",
      "Epoch 6: g_loss improved from 0.69870 to 0.69853, saving model to model_weights_128_2000_1e-06.h5\n",
      "Epoch: 6, D loss: 0.6889429688453674, G loss: 0.6985297799110413\n",
      "\n",
      "Epoch 7: g_loss did not improve from 0.69853\n",
      "Epoch: 7, D loss: 0.6907737851142883, G loss: 0.6993253231048584\n",
      "\n",
      "Epoch 8: g_loss did not improve from 0.69853\n",
      "Epoch: 8, D loss: 0.6907737851142883, G loss: 0.6995527744293213\n",
      "\n",
      "Epoch 9: g_loss did not improve from 0.69853\n",
      "Epoch: 9, D loss: 0.6907737851142883, G loss: 0.6991853713989258\n",
      "\n",
      "Epoch 10: g_loss did not improve from 0.69853\n",
      "Epoch: 10, D loss: 0.6897311508655548, G loss: 0.7001761794090271\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store the loss values for each combination of hyperparameters\n",
    "loss_values = {}\n",
    "\n",
    "data_shape = df_preprocessed.shape[1:]\n",
    "\n",
    "# Define the hyperparameters to experiment with\n",
    "batch_sizes = [32, 64, 128]\n",
    "latent_dims = [500 , 1000, 2000]\n",
    "learning_rates = [0.0001, 0.00001, 0.000001]\n",
    "\n",
    "# Initialize variables to store the best models and their hyperparameters\n",
    "best_gen_loss = float('inf')\n",
    "best_disc_loss = float('inf')\n",
    "best_generator = None\n",
    "best_discriminator = None\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Experiment with different batch sizes, latent dimensions, and learning rates\n",
    "for batch_size in batch_sizes:\n",
    "    for latent_dim in latent_dims:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"\\n\\nTraining with batch size {batch_size}, latent dimension {latent_dim}, and learning rate {learning_rate}\")\n",
    "\n",
    "            checkpoint_file = f'model_weights_{batch_size}_{latent_dim}_{learning_rate}.h5'\n",
    "            if os.path.exists(checkpoint_file):\n",
    "                print(f\"Checkpoint {checkpoint_file} exists, skipping training for this configuration\")\n",
    "                continue\n",
    "\n",
    "            generator = build_generator(latent_dim, data_shape)\n",
    "            discriminator = build_discriminator(data_shape)\n",
    "            gan = compile_gan(generator, discriminator, learning_rate=learning_rate)\n",
    "        \n",
    "            # Define the checkpoint callback\n",
    "            checkpoint = ModelCheckpoint(checkpoint_file, monitor='g_loss', verbose=1, save_best_only=True, mode='min')\n",
    "            checkpoint.set_model(gan)\n",
    "            callbacks_list = [checkpoint]\n",
    "\n",
    "            gen_loss, disc_loss = train_gan(generator, discriminator, gan, df_preprocessed, epochs=10, batch_size=batch_size, latent_dim=latent_dim, callbacks=callbacks_list)\n",
    "\n",
    "            # Store the loss values in the dictionary\n",
    "            loss_values[(batch_size, latent_dim, learning_rate)] = (gen_loss, disc_loss)\n",
    "\n",
    "            # If the generator loss is lower than the best seen so far, update the best models and their hyperparameters\n",
    "            if gen_loss[-1] < best_gen_loss and disc_loss[-1] < best_disc_loss:\n",
    "                best_gen_loss = gen_loss[-1]\n",
    "                best_disc_loss = disc_loss[-1]\n",
    "                best_generator = generator\n",
    "                best_discriminator = discriminator\n",
    "                best_hyperparameters = (batch_size, latent_dim, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the generator model to disk\n",
    "best_generator.save('generator.h5')\n",
    "\n",
    "# Save the discriminator model to disk\n",
    "best_discriminator.save('discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: batch size = 32, latent dimension = 500, learning rate = 0.0001\n",
      "Best generator loss: 1.0475250482559204\n",
      "Best discriminator loss: 0.43872401118278503\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best hyperparameters: batch size = {best_hyperparameters[0]}, latent dimension = {best_hyperparameters[1]}, learning rate = {best_hyperparameters[2]}\")\n",
    "print(f\"Best generator loss: {best_gen_loss}\")\n",
    "print(f\"Best discriminator loss: {best_disc_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'generator.h5' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Model Visualizer'''\n",
    "# Start the Netron model viewer for the generator model\n",
    "# netron.start('generator.h5')\n",
    "\n",
    "# Start the Netron model viewer for the discriminator model\n",
    "# netron.start('discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the generator model\n",
    "best_generator = load_model('generator.h5')\n",
    "\n",
    "# Load the discriminator model\n",
    "best_discriminator = load_model('discriminator.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(best_generator, num_samples, latent_dim):\n",
    "    # Generate noise\n",
    "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
    "    # Generate data\n",
    "    generated_data = best_generator.predict(noise)\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_generated_data_to_text(generated_data, preprocessor):\n",
    "    # Retrieve the transformers from the preprocessing pipeline\n",
    "    binary_transformer = preprocessor.named_transformers_['binary']\n",
    "    count_transformer = preprocessor.named_transformers_['count']\n",
    "    categorical_transformer = preprocessor.named_transformers_['cat']\n",
    "    standard_transformer = preprocessor.named_transformers_['num']\n",
    "\n",
    "    # Calculate the number of features for each transformer's output\n",
    "    num_binary_features = len(binary_transformer.get_feature_names_out())\n",
    "    num_count_features = 4 # If FunctionTransformer, this may need to be manually set\n",
    "    num_categorical_features = sum(len(cat) for cat in categorical_transformer.categories_)\n",
    "    num_standard_features = len(standard_transformer.feature_names_in_)\n",
    "\n",
    "    # Calculate the indices where each feature type starts and ends\n",
    "    end_binary = num_binary_features\n",
    "    start_count = end_binary\n",
    "    end_count = start_count + num_count_features\n",
    "    start_cat = end_count\n",
    "    end_cat = start_cat + num_categorical_features\n",
    "    start_standard = end_cat\n",
    "    end_standard = start_standard + num_standard_features\n",
    "\n",
    "    # Split the generated data back into its binary, count, categorical, and standard components\n",
    "    binary_data = generated_data[:, :end_binary]\n",
    "    count_data = generated_data[:, start_count:end_count]\n",
    "    categorical_data = generated_data[:, start_cat:end_cat]\n",
    "    standard_data = generated_data[:, start_standard:end_standard]\n",
    "\n",
    "    # Inverse transform the binary data\n",
    "    binary_data_inverse = binary_transformer.inverse_transform(binary_data)\n",
    "\n",
    "    # Inverse transform the count data\n",
    "    # Assuming count_transformer is a FunctionTransformer with np.log1p, inverse using np.expm1\n",
    "    count_data_inverse = np.expm1(count_data)\n",
    "\n",
    "    # Inverse transform the categorical data\n",
    "    categorical_data_inverse = categorical_transformer.inverse_transform(categorical_data)\n",
    "\n",
    "    # Inverse transform the standard data\n",
    "    standard_data_inverse = standard_transformer.inverse_transform(standard_data)\n",
    "\n",
    "    # Combine binary, count, categorical, and standard data\n",
    "    combined_data = np.hstack((binary_data_inverse, count_data_inverse, categorical_data_inverse, standard_data_inverse))\n",
    "\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 18s 105ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.3739289 ,  0.1942999 ,  0.44117308, ...,  0.3525878 ,\n",
       "       -0.3040509 ,  0.37937507], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "num_samples = 5000  # Number of samples you want to generate\n",
    "latent_dim = best_generator.layers[0].input_shape[1]\n",
    "generated_data = generate_data(best_generator, num_samples, latent_dim)\n",
    "generated_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 109ms/step\n"
     ]
    }
   ],
   "source": [
    "num_samples = 2000  # Number of samples you want to generate\n",
    "generated_data = generate_data(best_generator, num_samples, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PROTOCOL_MAP  L4_SRC_PORT   IPV4_SRC_ADDR  L4_DST_PORT   IPV4_DST_ADDR  \\\n",
      "0          tcp            0  10.114.225.204            0  10.114.225.206   \n",
      "\n",
      "   PROTOCOL  TCP_FLAGS  IN_BYTES  IN_PKTS  OUT_BYTES  OUT_PKTS  ANOMALY  \\\n",
      "0         6         16     36609    10074         10        12       11   \n",
      "\n",
      "   ALERT                  src_hierarchy                dst_hierarchy  \n",
      "0     -2  NI>Managua Department>Managua  US>California>Mountain View  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming 'preprocessor' is your ColumnTransformer instance\n",
    "combined_data = convert_generated_data_to_text(generated_data, preprocessor)\n",
    "\n",
    "# Convert combined_data to DataFrame for easier viewing/manipulation\n",
    "df_synthetic = pd.DataFrame(combined_data, columns=numerical_cols + categorical_cols + count_cols + binary_cols)\n",
    "\n",
    "df_synthetic = df_synthetic[['PROTOCOL_MAP', 'L4_SRC_PORT', 'IPV4_SRC_ADDR', 'L4_DST_PORT', 'IPV4_DST_ADDR', 'PROTOCOL', 'TCP_FLAGS', 'IN_BYTES', 'IN_PKTS', 'OUT_BYTES', 'OUT_PKTS', 'ANOMALY', 'ALERT', 'src_hierarchy', 'dst_hierarchy']]\n",
    "\n",
    "# Convert to numeric, errors='coerce' will set non-numeric values to NaN\n",
    "df_synthetic['L4_SRC_PORT'] = pd.to_numeric(df_synthetic['L4_SRC_PORT'], errors='coerce')\n",
    "df_synthetic['L4_DST_PORT'] = pd.to_numeric(df_synthetic['L4_DST_PORT'], errors='coerce')\n",
    "df_synthetic['PROTOCOL'] = pd.to_numeric(df_synthetic['PROTOCOL'], errors='coerce')\n",
    "df_synthetic['TCP_FLAGS'] = pd.to_numeric(df_synthetic['TCP_FLAGS'], errors='coerce')\n",
    "df_synthetic['IN_BYTES'] = pd.to_numeric(df_synthetic['IN_BYTES'], errors='coerce')\n",
    "df_synthetic['IN_PKTS'] = pd.to_numeric(df_synthetic['IN_PKTS'], errors='coerce')\n",
    "df_synthetic['OUT_BYTES'] = pd.to_numeric(df_synthetic['OUT_BYTES'], errors='coerce')\n",
    "df_synthetic['OUT_PKTS'] = pd.to_numeric(df_synthetic['OUT_PKTS'], errors='coerce')\n",
    "df_synthetic['ANOMALY'] = pd.to_numeric(df_synthetic['ANOMALY'], errors='coerce')\n",
    "df_synthetic['ALERT'] = pd.to_numeric(df_synthetic['ALERT'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# Apply rounding and convert to integer\n",
    "df_synthetic['L4_SRC_PORT'] = df_synthetic['L4_SRC_PORT'].round().astype(int)\n",
    "df_synthetic['L4_DST_PORT'] = df_synthetic['L4_DST_PORT'].round().astype(int)\n",
    "df_synthetic['PROTOCOL'] = df_synthetic['PROTOCOL'].round().astype(int)\n",
    "df_synthetic['TCP_FLAGS'] = df_synthetic['TCP_FLAGS'].round().astype(int)\n",
    "df_synthetic['IN_BYTES'] = df_synthetic['IN_BYTES'].round().astype(int)\n",
    "df_synthetic['IN_PKTS'] = df_synthetic['IN_PKTS'].round().astype(int)\n",
    "df_synthetic['OUT_BYTES'] = df_synthetic['OUT_BYTES'].round().astype(int)\n",
    "df_synthetic['OUT_PKTS'] = df_synthetic['OUT_PKTS'].round().astype(int)\n",
    "df_synthetic['ANOMALY'] = df_synthetic['ANOMALY'].round().astype(int)\n",
    "df_synthetic['ALERT'] = df_synthetic['ALERT'].round().astype(int)\n",
    "\n",
    "\n",
    "print(df_synthetic.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic.to_csv('synthetic_dataset_generated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
